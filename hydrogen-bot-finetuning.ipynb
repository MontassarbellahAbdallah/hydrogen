{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13010748,"sourceType":"datasetVersion","datasetId":8237165},{"sourceId":13019897,"sourceType":"datasetVersion","datasetId":8243397}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# finetuning ","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport os\n\nMODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\nOUTPUT_DIR = \"./phi3_medical_finetuned\"\nDATA_FILE = \"/kaggle/input/hydrogen/data.json\"\n\ndef load_and_prepare_data():\n    \"\"\"Charger et préparer les données\"\"\"\n    print(\"Chargement des données...\")\n    \n    with open(DATA_FILE, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    \n    prompts = []\n    for item in data:\n        user_input = item['texte']\n        classification = item['classification']\n        entites = item['entites']\n        \n        response = {\n            \"classification\": classification,\n            \"heure\": entites.get('heure', 'NON_SPECIFIE'),\n            \"date\": entites.get('date', 'NON_SPECIFIE'),\n            \"nom_docteur\": entites.get('nom_docteur', 'NON_SPECIFIE')\n        }\n        \n        prompt = f\"\"\"<|user|>\nAnalyse cette demande médicale et réponds en JSON:\n{user_input}\n<|end|>\n<|assistant|>\n{json.dumps(response, ensure_ascii=False)}\n<|end|>\"\"\"\n        \n        prompts.append(prompt)\n    \n    split_idx = int(len(prompts) * 0.8)\n    train_prompts = prompts[:split_idx]\n    val_prompts = prompts[split_idx:]\n    \n    print(f\"Train: {len(train_prompts)}, Validation: {len(val_prompts)}\")\n    return train_prompts, val_prompts\n\ndef setup_model():\n    print(\"Chargement du modèle Phi-3...\")\n    \n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.float32,  \n        trust_remote_code=True,\n    )\n    \n    if torch.cuda.is_available():\n        model = model.cuda()\n        print(f\"Modèle sur GPU: {torch.cuda.get_device_name()}\")\n    \n    for param in model.parameters():\n        param.requires_grad = False  \n    \n    # Configuration LoRA\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False,\n        r=16,  \n        lora_alpha=32,\n        lora_dropout=0.1,\n        target_modules=[\"qkv_proj\", \"o_proj\"]  \n    )\n    \n    # Appliquer LoRA\n    model = get_peft_model(model, lora_config)\n    \n    for name, param in model.named_parameters():\n        if 'lora_' in name:\n            param.requires_grad = True\n    \n    model.print_trainable_parameters()\n    \n    return model, tokenizer\n\ndef create_datasets(train_prompts, val_prompts, tokenizer):\n    \"\"\"Créer les datasets tokenizés\"\"\"\n    print(\"Création des datasets...\")\n    \n    def tokenize_function(examples):\n        \"\"\"Fonction de tokenization\"\"\"\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=True,\n            max_length=512,\n            return_tensors=None,\n        )\n        \n        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n        \n        return tokenized\n    \n    train_dataset = Dataset.from_dict({\"text\": train_prompts})\n    val_dataset = Dataset.from_dict({\"text\": val_prompts})\n    \n    print(\"Tokenization...\")\n    \n    train_dataset = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],\n        desc=\"Tokenizing train\"\n    )\n    \n    val_dataset = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],\n        desc=\"Tokenizing validation\"\n    )\n    \n    print(\"Datasets tokenizés\")\n    return train_dataset, val_dataset\n\ndef train_model(model, tokenizer, train_dataset, val_dataset):\n    print(\"Début de l'entraînement...\")\n    \n    trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n    print(f\"Paramètres entraînables: {len(trainable_params)}\")\n    \n    if len(trainable_params) == 0:\n        raise ValueError(\"Aucun paramètre entraînable trouvé!\")\n    \n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        num_train_epochs=3,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,  \n        warmup_steps=10,\n        logging_steps=5,\n        save_steps=50,\n        save_total_limit=1,\n        remove_unused_columns=False,\n        dataloader_pin_memory=False,\n        dataloader_num_workers=0,\n        report_to=[],\n        fp16=False,  \n        optim=\"adamw_torch\",\n        label_names=[\"labels\"],\n        gradient_checkpointing=False, \n    )\n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        processing_class=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Entraînement\n    trainer.train()\n    \n    # Sauvegarde\n    trainer.save_model()\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    \n    print(f\"Modèle sauvegardé dans {OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:55:10.906053Z","iopub.execute_input":"2025-09-10T14:55:10.906310Z","iopub.status.idle":"2025-09-10T14:55:37.709582Z","shell.execute_reply.started":"2025-09-10T14:55:10.906288Z","shell.execute_reply":"2025-09-10T14:55:37.709050Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def main():\n    print(\"FINE-TUNING PHI-3\")\n    print(\"=\" * 50)\n    \n    # Nettoyage mémoire\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    try:\n        if not os.path.exists(DATA_FILE):\n            print(f\"Fichier {DATA_FILE} non trouvé!\")\n            return\n        \n        print(f\"GPU disponible: {torch.cuda.is_available()}\")\n        \n        # Pipeline\n        train_prompts, val_prompts = load_and_prepare_data()\n        model, tokenizer = setup_model()\n        train_dataset, val_dataset = create_datasets(train_prompts, val_prompts, tokenizer)\n        train_model(model, tokenizer, train_dataset, val_dataset)\n        \n        print(\"FINE-TUNING TERMINÉ!\")\n        print(f\"Modèle sauvegardé: {OUTPUT_DIR}\")\n        \n    except Exception as e:\n        print(f\"Erreur: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:56:21.259570Z","iopub.execute_input":"2025-09-10T14:56:21.260392Z","iopub.status.idle":"2025-09-10T15:02:17.690681Z","shell.execute_reply.started":"2025-09-10T14:56:21.260360Z","shell.execute_reply":"2025-09-10T15:02:17.689954Z"}},"outputs":[{"name":"stdout","text":"FINE-TUNING PHI-3\n==================================================\nGPU disponible: True\nChargement des données...\n281 exemples chargés\nTrain: 224, Validation: 57\nChargement du modèle Phi-3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137c8f35b59b4aadb70e83f60571e431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d7eaf4c24145abbb7bfeb1043cdd8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc38c562a99483496b0707b8cf597d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea6063e1de846249602765ab7a24209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee9691cf9f441bebba90daa3341a959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6a1c9189a941239c784f57333b2257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01a3877de73c4db9acf9800be2a4108b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e5cd0fa135841f6b3d19b4a5a3de10e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c661e67eee4760baaadca8e1d2d743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ab386af72654ba09634b8573e2ffdfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b0e5d0023a34c53bb4f85c9df672449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed244b18952f4b5295c068a6c7418b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae5533c6ef14da38ffc8605cc7c91bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884d519ea55c4095a7ab2f8f4b3569cb"}},"metadata":{}},{"name":"stdout","text":"Modèle sur GPU: Tesla P100-PCIE-16GB\ntrainable params: 9,437,184 || all params: 3,830,516,736 || trainable%: 0.2464\nCréation des datasets...\nTokenization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train:   0%|          | 0/224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b41ce8579934739b4c6de16e42a6c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing validation:   0%|          | 0/57 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63670ed71af2451c8396505f51e41034"}},"metadata":{}},{"name":"stdout","text":"Datasets tokenizés\nDébut de l'entraînement...\nParamètres entraînables: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [168/168 04:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.371700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.940000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.342000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.715000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.545700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.445300</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.425800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.384900</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.379000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.364800</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.385000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.326900</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.358300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.301900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.268700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.289500</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.302300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.292400</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.330300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.314900</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.277400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.292400</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.290900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.260700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.261700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.255400</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.250400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.257900</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.254900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.258700</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.236600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.240200</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.232400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Modèle sauvegardé dans ./phi3_medical_finetuned\nFINE-TUNING TERMINÉ!\nModèle sauvegardé: ./phi3_medical_finetuned\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Utilisation de model Apres le finetuning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\nimport json\nimport re\n\nclass FinetunedModelTester:\n    def __init__(self, model_path):\n        self.model_path = model_path\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        try:\n            print(\"Chargement du modèle fine-tuné...\")\n            \n            # Charger la configuration LoRA\n            config = PeftConfig.from_pretrained(self.model_path)\n            \n            # Charger le tokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Charger le modèle de base\n            base_model = AutoModelForCausalLM.from_pretrained(\n                config.base_model_name_or_path,\n                torch_dtype=torch.float32,\n                trust_remote_code=True,\n                device_map=\"auto\", \n            )\n            \n            # Charger le modèle LoRA\n            self.model = PeftModel.from_pretrained(base_model, self.model_path)\n            self.model.eval()\n            \n            print(\"Modèle chargé avec succès!\")\n            return True\n            \n        except Exception as e:\n            print(f\"Erreur de chargement: {e}\")\n            return False\n    \n    def generate_response(self, user_input, max_tokens=120):\n        \n        prompt = f\"\"\"<|user|>\nAnalyse cette demande médicale et réponds en JSON:\n{user_input}\n<|end|>\n<|assistant|>\n\"\"\"\n        \n        try:\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True)\n            \n            if torch.cuda.is_available():\n                inputs = {k: v.cuda() for k, v in inputs.items()}\n            \n            input_ids = inputs[\"input_ids\"]\n            generated_text = \"\"\n            json_started = False\n            json_ended = False\n            \n            # Génération token par token avec détection de fin JSON\n            with torch.no_grad():\n                for i in range(max_tokens):\n                    outputs = self.model(input_ids)\n                    logits = outputs.logits\n                    \n                    next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0).unsqueeze(0)\n                    \n                    if next_token_id.item() == self.tokenizer.eos_token_id:\n                        break\n                    \n                    # Décoder le nouveau token\n                    new_token = self.tokenizer.decode(next_token_id.item(), skip_special_tokens=True)\n                    generated_text += new_token\n                    \n                    # Détecter le début du JSON\n                    if \"{\" in new_token:\n                        json_started = True\n                    \n                    # CONDITION D'ARRÊT : Si on trouve } \n                    if json_started and \"}\" in new_token:\n                        json_ended = True\n                        # Ajouter ce token et arrêter\n                        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n                        break\n                    \n                    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n                    \n                    if input_ids.shape[1] > 1000:\n                        break\n            \n            # Décoder la réponse finale\n            full_response = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n            \n            if \"<|assistant|>\" in full_response:\n                response = full_response.split(\"<|assistant|>\")[-1].strip()\n                if \"<|end|>\" in response:\n                    response = response.split(\"<|end|>\")[0].strip()\n                return response\n            else:\n                return full_response.replace(prompt, \"\").strip()\n                \n        except Exception as e:\n            return f\"Erreur de génération: {str(e)}\"\n    \n    def parse_json(self, response):\n        \"\"\"parser le JSON\"\"\"\n        try:\n            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n            if json_match:\n                json_str = json_match.group()\n                return json.loads(json_str)\n            else:\n                return None\n        except:\n            return None\n    \n    def test_examples(self):\n        \"\"\"des exemples prédéfinis\"\"\"\n        examples = [\n            \"Je veux un RDV avec Dr Martin demain à 14h\",\n            \"Message urgent pour le cardiologue Dr Mansouri\",\n            \"Je veux réserver une table au restaurant\",\n            \"Consultation avec Dr Sophie lundi matin\",\n            \"Laisser un message au dentiste Dr Khalil\"\n        ]\n        \n        print(\"=== TEST DU MODÈLE FINE-TUNÉ ===\\n\")\n        \n        for i, example in enumerate(examples, 1):\n            print(f\"TEST {i}: {example}\")\n            print(\"-\" * 50)\n            \n            # Générer la réponse\n            response = self.generate_response(example)\n            print(f\"RÉPONSE BRUTE:\")\n            print(response)\n            print()\n            \n            # Essayer de parser le JSON\n            parsed = self.parse_json(response)\n            if parsed:\n                print(\"JSON PARSÉ:\")\n                for key, value in parsed.items():\n                    print(f\"  {key}: {value}\")\n            else:\n                print(\"PAS DE JSON VALIDE DÉTECTÉ\")\n            \n            print(\"=\" * 60)\n            print()\n\ndef main():\n    model_path = \"/kaggle/input/phi3-finetuned-model/phi3_medical_finetuned\"\n    tester = FinetunedModelTester(model_path)\n    \n    # Charger le modèle\n    if not tester.load_model():\n        print(\"Impossible de charger le modèle. Arrêt du script.\")\n        return\n    \n    # Option 1: Tester avec des exemples prédéfinis\n    print(\"Voulez-vous tester avec des exemples prédéfinis ? (y/n)\")\n    choice = input().lower()\n    \n    if choice == 'y':\n        tester.test_examples()\n    \n    # Option 2: Tests interactifs\n    print(\"\\n=== MODE INTERACTIF ===\")\n    print(\"Tapez vos demandes (tapez 'quit' pour arrêter):\\n\")\n    \n    while True:\n        user_input = input(\"Votre demande: \")\n        \n        if user_input.lower() == 'quit':\n            break\n        \n        if user_input.strip():\n            print(\"\\nAnalyse en cours...\")\n            response = tester.generate_response(user_input)\n            \n            print(f\"\\nRÉPONSE DU MODÈLE:\")\n            print(response)\n            \n            # Parser le JSON\n            parsed = tester.parse_json(response)\n            if parsed:\n                print(f\"\\nRÉSULTAT STRUCTURÉ:\")\n                classification = parsed.get('classification', 'INCONNU')\n                print(f\"Classification: {classification}\")\n                \n                if classification == \"RESERVATION\":\n                    print(f\"Heure: {parsed.get('heure', 'NON_SPECIFIE')}\")\n                    print(f\"Date: {parsed.get('date', 'NON_SPECIFIE')}\")\n                    print(f\"Médecin: {parsed.get('nom_docteur', 'NON_SPECIFIE')}\")\n                elif classification == \"MESSAGE\":\n                    print(f\"Message pour: {parsed.get('nom_docteur', 'NON_SPECIFIE')}\")\n                elif classification == \"HORS_SUJET\":\n                    print(\"Demande hors domaine médical\")\n            else:\n                print(\"\\nPAS DE JSON VALIDE - Le modèle n'a pas appris le format correct\")\n            \n            print(\"-\" * 50)\n        else:\n            print(\"Veuillez saisir une demande valide.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:09:33.583960Z","iopub.execute_input":"2025-09-10T15:09:33.584269Z","iopub.status.idle":"2025-09-10T15:14:40.984766Z","shell.execute_reply.started":"2025-09-10T15:09:33.584247Z","shell.execute_reply":"2025-09-10T15:14:40.983845Z"}},"outputs":[{"name":"stdout","text":"Chargement du modèle fine-tuné...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb569af9367c4a1ca98b615bf3ba04d0"}},"metadata":{}},{"name":"stdout","text":"Modèle chargé avec succès!\nVoulez-vous tester avec des exemples prédéfinis ? (y/n)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" y\n"},{"name":"stdout","text":"=== TEST DU MODÈLE FINE-TUNÉ ===\n\nTEST 1: Je veux un RDV avec Dr Martin demain à 14h\n--------------------------------------------------\nRÉPONSE BRUTE:\nAnalyse cette demande médicale et réponds en JSON:\nJe veux un RDV avec Dr Martin demain à 14h\n {\"classification\": \"RESERVATION\", \"heure\": \"14h\", \"date\": \"demain\", \"nom_docteur\": \"Dr Martin\"}\n\nJSON PARSÉ:\n  classification: RESERVATION\n  heure: 14h\n  date: demain\n  nom_docteur: Dr Martin\n============================================================\n\nTEST 2: Message urgent pour le cardiologue Dr Mansouri\n--------------------------------------------------\nRÉPONSE BRUTE:\nAnalyse cette demande médicale et réponds en JSON:\nMessage urgent pour le cardiologue Dr Mansouri\n {\"classification\": \"MESSAGE\", \"heure\": \"NON_SPECIFIE\", \"date\": \"NON_SPECIFIE\", \"nom_docteur\": \"Dr Mansouri\"}\n\nJSON PARSÉ:\n  classification: MESSAGE\n  heure: NON_SPECIFIE\n  date: NON_SPECIFIE\n  nom_docteur: Dr Mansouri\n============================================================\n\nTEST 3: Je veux réserver une table au restaurant\n--------------------------------------------------\nRÉPONSE BRUTE:\nAnalyse cette demande médicale et réponds en JSON:\nJe veux réserver une table au restaurant\n {\"classification\": \"HORS_SUJET\", \"heure\": \"NON_SPECIFIE\", \"date\": \"NON_SPECIFIE\", \"nom_docteur\": \"NON_SPECIFIE\"}\n\nJSON PARSÉ:\n  classification: HORS_SUJET\n  heure: NON_SPECIFIE\n  date: NON_SPECIFIE\n  nom_docteur: NON_SPECIFIE\n============================================================\n\nTEST 4: Consultation avec Dr Sophie lundi matin\n--------------------------------------------------\nRÉPONSE BRUTE:\nAnalyse cette demande médicale et réponds en JSON:\nConsultation avec Dr Sophie lundi matin\n {\"classification\": \"RESERVATION\", \"heure\": \"matin\", \"date\": \"lundi\", \"nom_docteur\": \"Dr Sophie\"}\n\nJSON PARSÉ:\n  classification: RESERVATION\n  heure: matin\n  date: lundi\n  nom_docteur: Dr Sophie\n============================================================\n\nTEST 5: Laisser un message au dentiste Dr Khalil\n--------------------------------------------------\nRÉPONSE BRUTE:\nAnalyse cette demande médicale et réponds en JSON:\nLaisser un message au dentiste Dr Khalil\n {\"classification\": \"MESSAGE\", \"heure\": \"NON_SPECIFIE\", \"date\": \"NON_SPECIFIE\", \"nom_docteur\": \"Dr Khalil\"}\n\nJSON PARSÉ:\n  classification: MESSAGE\n  heure: NON_SPECIFIE\n  date: NON_SPECIFIE\n  nom_docteur: Dr Khalil\n============================================================\n\n\n=== MODE INTERACTIF ===\nTapez vos demandes (tapez 'quit' pour arrêter):\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Votre demande:  je veux faire une reservation de 5 places de table\n"},{"name":"stdout","text":"\nAnalyse en cours...\n\nRÉPONSE DU MODÈLE:\nAnalyse cette demande médicale et réponds en JSON:\nje veux faire une reservation de 5 places de table\n {\"classification\": \"HORS_SUJET\", \"heure\": \"NON_SPECIFIE\", \"date\": \"NON_SPECIFIE\", \"nom_docteur\": \"NON_SPECIFIE\"}\n\nRÉSULTAT STRUCTURÉ:\nClassification: HORS_SUJET\nDemande hors domaine médical\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Votre demande:  quit\n"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}